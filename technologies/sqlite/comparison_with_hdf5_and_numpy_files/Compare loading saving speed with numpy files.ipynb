{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare data saving and loading performance of QCoDeS SQLite backend vs HDF5 (h5py) and numpy npy files\n",
    "\n",
    "This notebook measures time it takes to save and load measurement data using qcodes dataset versus other ways of storing data, hdf5 and numpy npy files. The reason for such a study is that qcodes users should not be limited in the performance of the their experiments by the performance of data saving (and loading).\n",
    "\n",
    "HDF5 and numpy npy storage solutions are widely used in the scientific community, and are known for their efficiency.\n",
    "\n",
    "In this notebook, we are going to define convenient functions that generate data, load and save that data in the means of interest, and some infrastructure that allows us to measure the time the loading and saving takes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging hadn't been started.\n",
      "Activating auto-logging. Current session state plus future input saved.\n",
      "Filename       : C:\\Users\\Jens-Work\\.qcodes\\logs\\command_history.log\n",
      "Mode           : append\n",
      "Output logging : True\n",
      "Raw input log  : False\n",
      "Timestamping   : True\n",
      "State          : active\n",
      "Qcodes Logfile : C:\\Users\\Jens-Work\\.qcodes\\logs\\200602-13884-qcodes.log\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from tempfile import TemporaryFile\n",
    "from functools import partial\n",
    "\n",
    "import numpy\n",
    "import h5py\n",
    "from git import Repo\n",
    "\n",
    "import qcodes\n",
    "from qcodes import (\n",
    "    initialise_or_create_database_at, load_or_create_experiment, \n",
    "    Measurement, Parameter,\n",
    "    load_by_id\n",
    ")\n",
    "from qcodes.dataset.data_export import get_data_by_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant environment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.0+107.g12c917c30'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qcodes.version.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12c917c30ceaa0a1afe5d7cccccdca71b2276c37\n"
     ]
    }
   ],
   "source": [
    "# in case the qcodes is installed from local git repository\n",
    "qcodes_repo_path = os.sep.join(qcodes.__path__[0].split(os.sep)[:-1])\n",
    "qcodes_repo = Repo(qcodes_repo_path)\n",
    "print(qcodes_repo.head.commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the h5py configuration\n",
      "---------------------------------\n",
      "\n",
      "h5py    2.10.0\n",
      "HDF5    1.10.5\n",
      "Python  3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)]\n",
      "sys.platform    win32\n",
      "sys.maxsize     9223372036854775807\n",
      "numpy   1.17.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(h5py.version.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated measurement\n",
    "\n",
    "For this study, we are going to take the case of sweeping 2 independent parameters (s1, s2) and measuring 2 dependent parameters (magnitude and phase). For simplicity, the number of datapoints per parameter is the same, and it is set in a variable. We are going to use the same generator function throughout the study for generating dummy data that we will be saving and loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of data points per parameter\n",
    "n_pts_per_param = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_producer(n_pts_per_param):\n",
    "    def produce_measurement_data():\n",
    "        \"\"\"\n",
    "        This iterator represents the code that obtains\n",
    "        measurement data. For the sake of example, it\n",
    "        just returns random dummy data: 4 parameters/dimensions, \n",
    "        `n_pts_per_param` per each dimension (which becomes\n",
    "        `n_pts_per_param**4` data points in total).\n",
    "\n",
    "        Args:\n",
    "            n_pts_per_param\n",
    "                number of points per each parameter/dimension\n",
    "\n",
    "        Returns:\n",
    "            tuple of values of the 4 dimensions obtained\n",
    "            at a single \"measurement\" iteration\n",
    "        \"\"\"\n",
    "        for s1_val in range(n_pts_per_param):\n",
    "            for s2_val in range(n_pts_per_param):\n",
    "                magn_vals, phas_vals = numpy.meshgrid(\n",
    "                    numpy.random.rand(n_pts_per_param),\n",
    "                    numpy.random.rand(n_pts_per_param),\n",
    "                )\n",
    "                magn_vals = numpy.reshape(magn_vals, -1)\n",
    "                phas_vals = numpy.reshape(phas_vals, -1)\n",
    "\n",
    "                yield s1_val, s2_val, magn_vals, phas_vals\n",
    "    return produce_measurement_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring execution time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most of the cases, we are going to use `timeit` to measure time.\n",
    "\n",
    "In some cases, however, `timeit` interface is not flexible: it does not let you measure \"start\" and \"stop\" time moments __within__ the code that is under test. Below is a custom decorator that allows to overcome this limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from IPython.core.magics.execution import TimeitResult\n",
    "from copy import deepcopy, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_it(number=None,\n",
    "            repeat=timeit.default_repeat):\n",
    "    \"\"\"\n",
    "    Sometimes it is needed to define in the code itself\n",
    "    where you want to start measuring the execution time\n",
    "    of that piece of code and when you want to stop the \n",
    "    measurement. Unfortunately, `timeit` module does not\n",
    "    support that out-of-the-box. Hence, this decorator.\n",
    "    \n",
    "    This decorator uses `timeit` infrastructure, but allows\n",
    "    to profile a function that returns its execution time.\n",
    "    This allows developers to define the start and stop moments\n",
    "    in the code itself, and the `timeit` infrastructure will\n",
    "    do the rest.\n",
    "    \n",
    "    To use this decorator, follow these steps:\n",
    "    * implement a piece of code that you'd like to profile\n",
    "      as a function\n",
    "    * in the code of the function find the start and stop\n",
    "      points where the time needs to be measured\n",
    "    * use `time.perf_counter()` to get the time in seconds\n",
    "      at those places\n",
    "    * make the function return the difference between stop\n",
    "      and start moments and its first return value\n",
    "    * the function signature is not restricted to its input\n",
    "      arguments, and is not restricted to its return values\n",
    "      except for the first return value\n",
    "    * decorate the function with this decorator\n",
    "    * call your decorated function to see the results of \n",
    "      the profiling\n",
    "    \n",
    "    Args:\n",
    "        number\n",
    "            the function gets executed this `number` of times,\n",
    "            and the average of the collected individual execution\n",
    "            times is used (same as for `timeit`); if None, then\n",
    "            the necessary number of execution times will be \n",
    "            inferred (see `timeit` module for more info)\n",
    "        repeat\n",
    "            the profiling measurement gets repeated `repeat`\n",
    "            number of times (same as for `timeit`)\n",
    "    \"\"\"\n",
    "    def time_sut(sut):\n",
    "        \"\"\"\n",
    "        This is the actual decorator. \"sut\" stands for \"system\n",
    "        under test\".\n",
    "        \"\"\"\n",
    "        def wrapper(*args, **kwargs):\n",
    "            \"\"\"\n",
    "            This wrapper function uses `timeit` infrastructure\n",
    "            from `timeit` module and its implementation in Jupyter\n",
    "            magics.\n",
    "            \n",
    "            Returns the `TimeitResult` object that contains all the\n",
    "            information about the profiling results.\n",
    "            \"\"\"\n",
    "            t = timeit.Timer()\n",
    "            \n",
    "            # define a function that the Timer class\n",
    "            # can consume for profiling\n",
    "            def inner(_it, _timer):\n",
    "                \"\"\"\n",
    "                see the internals of the `timeit.Timer` class\n",
    "                for more information\n",
    "                \"\"\"\n",
    "                total_time = 0\n",
    "                for _ in _it:\n",
    "                    args_ = copy(args)\n",
    "                    kwargs_ = copy(kwargs)\n",
    "                    \n",
    "                    returned_vals = sut(*args_, **kwargs_)\n",
    "                    \n",
    "                    total_time += returned_vals[0] \\\n",
    "                                      if isinstance(returned_vals, tuple) \\\n",
    "                                  else returned_vals\n",
    "                return total_time\n",
    "            \n",
    "            t.inner = inner\n",
    "            \n",
    "            # execute the profiling\n",
    "            try:\n",
    "                if number is None:\n",
    "                    number_, __ = t.autorange()\n",
    "                else:\n",
    "                    number_ = number\n",
    "                \n",
    "                all_runs = t.repeat(repeat, number_)\n",
    "            except:\n",
    "                t.print_exc()\n",
    "                raise\n",
    "            \n",
    "            # pretty print the results\n",
    "            best = min(all_runs) / number_\n",
    "            worst = max(all_runs) / number_\n",
    "            timeit_result = TimeitResult(number_, repeat, best, worst, all_runs, 0, 3)\n",
    "            print(timeit_result)\n",
    "            \n",
    "            return timeit_result\n",
    "        return wrapper\n",
    "    return time_sut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining test routines\n",
    "\n",
    "Now lets define all the test routines for saving and loading data for testing the performance of different backends. These routines will use the data generation function that is defined above, and some of them will conform to the interface that is required by the custom `time_it` decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QCoDeS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize a database file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upgrading database; v0 -> v1: : 0it [00:00, ?it/s]\n",
      "Upgrading database; v1 -> v2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 499.50it/s]\n",
      "Upgrading database; v2 -> v3: : 0it [00:00, ?it/s]\n",
      "Upgrading database; v3 -> v4: : 0it [00:00, ?it/s]\n",
      "Upgrading database; v4 -> v5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 499.20it/s]\n",
      "Upgrading database; v5 -> v6: : 0it [00:00, ?it/s]\n",
      "Upgrading database; v6 -> v7: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Upgrading database; v7 -> v8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 499.68it/s]\n",
      "Upgrading database; v8 -> v9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 500.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "save_load_speed_study#sqlite3_from_qcodes#1@C:\\Users\\JENS-W~1\\AppData\\Local\\Temp\\tmp6dumqcbs.db\n",
       "-----------------------------------------------------------------------------------------------"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the database file for qcodes dataset\n",
    "\n",
    "temp_db_file = TemporaryFile(suffix='.db')\n",
    "temp_db_file.close()\n",
    "\n",
    "initialise_or_create_database_at(temp_db_file.name)\n",
    "\n",
    "load_or_create_experiment('save_load_speed_study', 'sqlite3_from_qcodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a convenient function that performs all the usual steps that are necessary for a qcodes measurement and data saving.\n",
    "\n",
    "Note that we exlcude from the time measurement the parts that are related to setting up the `Measurement` object, and starting the actual measurement. We do include the exiting of the `measurement.run()` context though because the last pieces of data are flushed then.\n",
    "\n",
    "We decorate it with the our custom `time_it` decorator that has been presented above (note that we want to keep the original function as well, hence the `@` syntax is not used for decoration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_sqlite(create_data_generator, \n",
    "                   paramtype='numeric', \n",
    "                   write_period=10):\n",
    "    \"\"\"\n",
    "    Use qcodes dataset with its sqlite backend to save dummy\n",
    "    data, and measure the time this takes. The data that is being\n",
    "    saved is 2 dependent and 2 independent parameters. The data\n",
    "    for the measurement is generated by an iterator that is returned\n",
    "    by calling the `create_data_generator` function.\n",
    "    \n",
    "    Args:\n",
    "        create_data_generator\n",
    "            a callable with not arguments that returns an iterator \n",
    "            that in turn generates dummy data for 4 parameters\n",
    "        paramtype\n",
    "            controls the way data the 2 dependent parameters are stored\n",
    "            in the sqlite database,\n",
    "            see `Measurement.register_parameter` for more information\n",
    "            (useful values in the context of this notebook are 'numeric'\n",
    "            and 'array')\n",
    "        write_period\n",
    "            the data is written to the data base at least every \n",
    "            `write_period` number of seconds\n",
    "            \n",
    "    Returns:\n",
    "        saving_time\n",
    "            measured time it took to save the data, in seconds\n",
    "        dataset\n",
    "            the qcodes dataset object where the data was saved to;\n",
    "            it is useful for accessing the data and measuring the\n",
    "            time it takes to load it\n",
    "    \"\"\"\n",
    "    data_generator = create_data_generator()\n",
    "    \n",
    "    # define parameters\n",
    "    s1 = Parameter('s1', label='Setting 1', unit='V', get_cmd=None, set_cmd=None)\n",
    "    s2 = Parameter('s2', label='Setting 2', unit='V', get_cmd=None, set_cmd=None)\n",
    "    magn = Parameter('magn', label='Magnitude', unit='V', get_cmd=None, set_cmd=None)\n",
    "    phas = Parameter('phas', label='Phase', unit='deg', get_cmd=None, set_cmd=None)\n",
    "    \n",
    "    meas = Measurement()\n",
    "    \n",
    "    # register parameters in the measurement object\n",
    "    meas.register_parameter(s1)\n",
    "    meas.register_parameter(s2)\n",
    "    meas.register_parameter(magn, setpoints=(s1, s2), paramtype=paramtype)\n",
    "    meas.register_parameter(phas, setpoints=(s1, s2), paramtype=paramtype)\n",
    "    \n",
    "    # set the write period to a large value, so that actual writing\n",
    "    # to the database happens at the end of the \"measurement\"\n",
    "    meas.write_period = write_period\n",
    "    \n",
    "    # perform the measurement\n",
    "    with meas.run() as datasaver:\n",
    "        t0 = time.perf_counter()  # <-----\n",
    "\n",
    "        for s1_val, s2_val, magn_vals, phas_vals \\\n",
    "                in data_generator:\n",
    "            \n",
    "            datasaver.add_result((s1, s1_val), \n",
    "                                 (s2, s2_val),\n",
    "                                 (magn, magn_vals),\n",
    "                                 (phas, phas_vals))\n",
    "    \n",
    "    t1 = time.perf_counter()  # <-----\n",
    "    saving_time = t1 - t0\n",
    "    \n",
    "    dataset = datasaver.dataset\n",
    "    \n",
    "    return saving_time, dataset\n",
    "\n",
    "\n",
    "# decorate the function, and leave the original one intact\n",
    "time_save_to_sqlite_numeric = time_it(number=3, repeat=2)(\n",
    "    partial(save_to_sqlite, paramtype='numeric'))\n",
    "\n",
    "time_save_to_sqlite_array = time_it()(\n",
    "    partial(save_to_sqlite, paramtype='array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5 file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 files (thanks for `h5py`) behave very similar to `numpy` array, the interfacing with them is very similar.\n",
    "\n",
    "We are not going to use the custom `time_it` decorator, because `timeit` itself is not limiting us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_hdf5(create_data_generator, filename):\n",
    "    \"\"\"\n",
    "    Use HDF5 file to save dummy data, and measure the time \n",
    "    this takes. The data that is being saved is 2 dependent\n",
    "    and 2 independent parameters. The resulting HDF5 file\n",
    "    is going to contain a single 'dataset' with the name\n",
    "    \"results\".\n",
    "    \n",
    "    The data for the measurement is generated by an iterator \n",
    "    that is returned by calling the `create_data_generator` \n",
    "    function.\n",
    "    \n",
    "    Args:\n",
    "        create_data_generator\n",
    "            a callable with not arguments that returns an iterator \n",
    "            that in turn generates dummy data for 4 parameters\n",
    "        filename\n",
    "            the name of the HDF5 file with the full path\n",
    "    \n",
    "    Returns:\n",
    "        saving_time\n",
    "            measured time it took to save the data, in seconds\n",
    "    \"\"\"\n",
    "    data_generator = create_data_generator()\n",
    "\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        ds = f.create_dataset('results', shape=(4, 0), maxshape=(4, None))\n",
    "\n",
    "        for s1_val, s2_val, magn_vals, phas_vals in data_generator:\n",
    "            n_pts = len(magn_vals)\n",
    "            \n",
    "            # we simulate the fact that we don't \n",
    "            # know the full amount of data\n",
    "            # that needs to be saved, hence \n",
    "            # we need to resize while saving\n",
    "            n_cols, n_rows = ds.shape\n",
    "            ds.resize((n_cols, n_rows + n_pts))\n",
    "\n",
    "            ds[0, n_rows:n_rows+n_pts] = s1_val\n",
    "            ds[1, n_rows:n_rows+n_pts] = s2_val\n",
    "            ds[2, n_rows:n_rows+n_pts] = magn_vals\n",
    "            ds[3, n_rows:n_rows+n_pts] = phas_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy npy file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use `numpy`'s `.npy` files together with the handy `open_memmap` function in order to save data that is being spit out of the iterator that generates data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_npy(create_data_generator, filename):\n",
    "    \"\"\"\n",
    "    Use numpy npy file to save dummy data, and measure the time \n",
    "    this takes. The data that is being saved is 2 dependent\n",
    "    and 2 independent parameters. The data for the measurement\n",
    "    is generated by an iterator that is returned\n",
    "    by calling the `create_data_generator` function.\n",
    "    \n",
    "    Args:\n",
    "        create_data_generator\n",
    "            a callable with not arguments that returns an iterator \n",
    "            that in turn generates dummy data for 4 parameters\n",
    "        filename\n",
    "            the name of the npy file with the full path; it has to\n",
    "            contain '.npy' extension, otherwise `numpy` will add it\n",
    "            when saving data, and it will be impossible to refer\n",
    "            to the actual file without manually appending the \n",
    "            '.npy' extension to the `filename` in the code\n",
    "            outside of this function\n",
    "    \n",
    "    Returns:\n",
    "        saving_time\n",
    "            measured time it took to save the data, in seconds\n",
    "    \"\"\"\n",
    "    data_generator = create_data_generator()\n",
    "\n",
    "    npy_mm = numpy.lib.format.open_memmap(\n",
    "        filename, mode='w+', shape=(4, 0))\n",
    "    \n",
    "    # this (possibly dangerous?) hack is needed to allow\n",
    "    # resizing during adding data\n",
    "    npy_mm = numpy.require(npy_mm, requirements=['OWNDATA'])\n",
    "\n",
    "    for s1_val, s2_val, magn_vals, phas_vals in data_generator:\n",
    "        n_pts = len(magn_vals)\n",
    "        \n",
    "        # we simulate the fact that we don't \n",
    "        # know the full amount of data\n",
    "        # that needs to be saved, hence \n",
    "        # we need to resize while saving\n",
    "        n_cols, n_rows = npy_mm.shape\n",
    "        npy_mm.resize((n_cols, n_rows + n_pts))\n",
    "\n",
    "        npy_mm[0, n_rows:n_rows+n_pts] = s1_val\n",
    "        npy_mm[1, n_rows:n_rows+n_pts] = s2_val\n",
    "        npy_mm[2, n_rows:n_rows+n_pts] = magn_vals\n",
    "        npy_mm[3, n_rows:n_rows+n_pts] = phas_vals\n",
    "\n",
    "    del npy_mm  # closes the file and performs final flushing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure saving times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save time of QCoDeS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with 'numeric' type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experimental run with id: 1. \n",
      "Starting experimental run with id: 2. \n",
      "Starting experimental run with id: 3. \n",
      "Starting experimental run with id: 4. \n",
      "Starting experimental run with id: 5. \n",
      "Starting experimental run with id: 6. \n",
      "1.86 s ± 1.26 ms per loop (mean ± std. dev. of 2 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "save_time_dataset_numeric = time_save_to_sqlite_numeric(\n",
    "    make_data_producer(n_pts_per_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saving to dataset with 'numeric' paramtype took:\n",
      "1.86 s ± 1.26 ms per loop (mean ± std. dev. of 2 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data saving to dataset with 'numeric' paramtype took:\")\n",
    "print(save_time_dataset_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with 'array' type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experimental run with id: 7. \n",
      "Starting experimental run with id: 8. \n",
      "Starting experimental run with id: 9. \n",
      "Starting experimental run with id: 10. \n",
      "Starting experimental run with id: 11. \n",
      "Starting experimental run with id: 12. \n",
      "Starting experimental run with id: 13. \n",
      "Starting experimental run with id: 14. \n",
      "Starting experimental run with id: 15. \n",
      "Starting experimental run with id: 16. \n",
      "Starting experimental run with id: 17. \n",
      "Starting experimental run with id: 18. \n",
      "Starting experimental run with id: 19. \n",
      "155 ms ± 4.06 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "save_time_dataset_array = time_save_to_sqlite_array(\n",
    "    make_data_producer(n_pts_per_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saving to dataset with 'array' paramtype took:\n",
      "155 ms ± 4.06 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data saving to dataset with 'array' paramtype took:\")\n",
    "print(save_time_dataset_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save time of HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 ms ± 2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit outfile = TemporaryFile(); outfile.close()\n",
    "\n",
    "save_to_hdf5(make_data_producer(n_pts_per_param), outfile.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save time of npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398 ms ± 5.39 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit outfile = TemporaryFile(suffix='.npy'); outfile.close()\n",
    "\n",
    "save_to_npy(make_data_producer(n_pts_per_param), outfile.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure loading times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load time of QCoDeS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QCoDeS dataset has two ways of just loading the data: via `DataSet.get_data` method, and via `get_data_by_id` function.\n",
    "\n",
    "We are going to use both, but note that `get_data_by_id` does a bit more than just loading the data, hence it is supposedly more popular among users.\n",
    "\n",
    "A third way is to use `DataSet.get_values` and obtain values of each parameter one by one. `get_data_by_id` is already using it internally, hence we are not going to profile it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### of 'numeric' type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experimental run with id: 20. \n"
     ]
    }
   ],
   "source": [
    "_, dataset_numeric = save_to_sqlite(\n",
    "    make_data_producer(n_pts_per_param), \n",
    "    paramtype='numeric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-02 09:40:47,478 ¦ py.warnings ¦ WARNING ¦ warnings ¦ _showwarnmsg ¦ 110 ¦ c:\\users\\jens-work\\source\\repos\\qcodes\\qcodes\\utils\\deprecate.py:58: QCoDeSDeprecationWarning: The function <get_data> is deprecated, because This method does not accurately represent the dataset.. Use \"Use `get_parameter_data` instead.\" as an alternative.\n",
      "  issue_deprecation_warning(f'{t} <{n}>', reason, alternative)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5 s ± 44.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit parameter_names = dataset_numeric.parameters.split(',')\n",
    "\n",
    "data = dataset_numeric.get_data(*parameter_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.56 s ± 22.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "data = get_data_by_id(dataset_numeric.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### of 'array' type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experimental run with id: 21. \n"
     ]
    }
   ],
   "source": [
    "_, dataset_array = save_to_sqlite(\n",
    "    make_data_producer(n_pts_per_param), \n",
    "    paramtype='array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 ms ± 509 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit parameter_names = dataset_array.parameters.split(',')\n",
    "\n",
    "data = dataset_array.get_data(*parameter_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 ms ± 648 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "data = get_data_by_id(dataset_array.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load time from HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdf5file = TemporaryFile()\n",
    "hdf5file.close()\n",
    "hdf5filename = hdf5file.name\n",
    "\n",
    "_ = save_to_hdf5(make_data_producer(n_pts_per_param), hdf5filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.24 ms ± 35.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "with h5py.File(hdf5filename, 'r') as f:\n",
    "    data = numpy.array(f['results'], copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load time from npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "npyfile = TemporaryFile(suffix='.npy')\n",
    "npyfile.close()\n",
    "npyfilename = npyfile.name\n",
    "\n",
    "_ = save_to_npy(make_data_producer(n_pts_per_param), npyfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251 µs ± 610 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "data = numpy.load(npyfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize the measurement results. \n",
    "\n",
    "Note that the results are added manually so that playing around with the cells of this notebook will not result in changes to the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For saving `4 columns` of data, `160000 (20**4)` points per each column, in a `for` loop over the first 2 parameters where the data for the last 2 parameters is obtained as arrays of `400 (20**2)` elements, assuming that the data is saved in these chunks, these are the measurement results for saving that data and loading it for different backends:\n",
    "\n",
    "| Backend | QCoDeS<br>dataset<br>'numeric' | QCoDeS<br>dataset<br>'array' | HDF5<br>file | .npy<br>file |\n",
    "|---|---|---|---|---|\n",
    "| Saving time  | ~ 1900 ms | ~ 160 ms | ~ 180 ms | ~ 400 ms |\n",
    "| Loading time | ~ 2500 ms | ~ 150 ms |  ~ 5 ms | ~ 0.25 ms |\n",
    "\n",
    "__Note__ that the numbers in this table have the `~` sign - this means that these numbers should be read as __\"on the order of\"__. The reason being is that when you run the cells of this notebook multiple times, you might get a discrepancy of ~10-20% from these numbers, but (hopefully) not more.\n",
    "\n",
    "__Conclusions__\n",
    "* 'numeric' type for QCoDeS dataset should not be used for large data\n",
    "* saving speed of 'array' type for QCoDeS dataset is similar to that of HDF5\n",
    "* loading time of HDF5 and npy is significantly better than that of QCoDeS dataset\n",
    "* saving speed of npy may seem large but that is most probably due to the resizing of the file on-the-fly while new data comes in; HDF5 seems to handle this 2x better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
